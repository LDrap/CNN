{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LDrap/CNN/blob/main/DeepNetsForEO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQIA0PcunJI"
      },
      "source": [
        "\n",
        "# Semantic segmentation of aerial images with deep networks\n",
        "\n",
        "This notebook presents a straightforward PyTorch implementation of a Fully Convolutional Network for semantic segmentation of aerial images. More specifically, we aim to automatically perform scene interpretation of images taken from a plane or a satellite by classifying every pixel into several land cover classes.\n",
        "\n",
        "As a demonstration, we are going to use the [SegNet architecture](http://mi.eng.cam.ac.uk/projects/segnet/) to segment aerial images over the cities of Vaihingen and Potsdam. The images are from the [ISPRS 2D Semantic Labeling dataset](http://www2.isprs.org/commissions/comm3/wg4/results.html). We will train a network to segment roads, buildings, vegetation and cars.\n",
        "\n",
        "This work is a PyTorch implementation of the baseline presented in \"[Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal Deep Networks](https://hal.archives-ouvertes.fr/hal-01636145) \", Nicolas Audebert, Bertrand Le Saux and Sébastien Lefèvre, ISPRS Journal, 2018.\n",
        "\n",
        "The original code for this notebook is the [DeepNetsForEO](https://github.com/nshaud/DeepNetsForEO) repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crpHH5AQCSBv"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Enable GPU with `Runtime->Change runtime type->Hardware Accelerator->GPU` in the top menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMHN1FoRuUfH"
      },
      "source": [
        "# imports and stuff\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import itertools\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler\n",
        "import torch.nn.init\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZScpjpH7uda",
        "outputId": "ea5fe99c-4500-4c3c-b3c6-639c03b3e300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# mount your google drive (follow the link, allow what is needed and get the authorization code)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "\n",
        "# data are assumed to be in Vaihingen/ directory, otherwise modify accordingly\n",
        "FOLDER = 'gdrive/My Drive/JURSE/'\n",
        "sys.path.append(FOLDER)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEWsRUno-5Ba"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "\n",
        "There are several parameters than can be tuned to use this notebook with different datasets. The default parameters are suitable for the ISPRS dataset, but you can change them to work with your data.\n",
        "\n",
        "## Examples\n",
        "\n",
        "*   Binary classification: N_CLASSES = 2\n",
        "*   Multi-spectral data (e.g. IRRGB): IN_CHANNELS = 4\n",
        "*  New folder naming convention : DATA_FOLDER = MAIN_FOLDER + 'sentinel2/sentinel2_img_{}.tif'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fterOFMwueZc"
      },
      "source": [
        "# Parameters++++\n",
        "WINDOW_SIZE = (256, 256) # Patch size\n",
        "STRIDE = 32 # Stride for testing\n",
        "IN_CHANNELS = 3 # Number of input channels (e.g. RGB)\n",
        "##FOLDER = \"./ISPRS_dataset/\" # Replace with your \"/path/to/the/ISPRS/dataset/folder/\"\n",
        "BATCH_SIZE = 5 # Number of samples in a mini-batch\n",
        "\n",
        "LABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"] # Label names\n",
        "N_CLASSES = len(LABELS) # Number of classes\n",
        "WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n",
        "CACHE = True # Store the dataset in-memory\n",
        "\n",
        "DATASET = 'Vaihingen'\n",
        "\n",
        "if DATASET == 'Potsdam':\n",
        "    MAIN_FOLDER = FOLDER + 'Potsdam/'\n",
        "    # Uncomment the next line for IRRG data\n",
        "    # DATA_FOLDER = MAIN_FOLDER + '3_Ortho_IRRG/top_potsdam_{}_IRRG.tif'\n",
        "    # For RGB data\n",
        "    DATA_FOLDER = MAIN_FOLDER + '2_Ortho_RGB/top_potsdam_{}_RGB.tif'\n",
        "    LABEL_FOLDER = MAIN_FOLDER + '5_Labels_for_participants/top_potsdam_{}_label.tif'\n",
        "    ERODED_FOLDER = MAIN_FOLDER + '5_Labels_for_participants_no_Boundary/top_potsdam_{}_label_noBoundary.tif'    \n",
        "elif DATASET == 'Vaihingen':\n",
        "    MAIN_FOLDER = FOLDER + 'Vaihingen/'\n",
        "    DATA_FOLDER = MAIN_FOLDER + 'top/top_mosaic_09cm_area1.tif'\n",
        "    LABEL_FOLDER = MAIN_FOLDER + 'gts_for_participants/top_mosaic_09cm_area1.tif'\n",
        "    ERODED_FOLDER = MAIN_FOLDER + 'gts_eroded_for_participants/top_mosaic_09cm_area1_noBoundary.tif'\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRs2xwrU_TrG"
      },
      "source": [
        "\n",
        "# Visualizing the dataset\n",
        "\n",
        "First, let's check that we are able to access the dataset and see what's going on. We use scikit-image for image manipulation.\n",
        "\n",
        "As the ISPRS dataset is stored with a ground truth in the RGB format, we need to define the color palette that can map the label id to its RGB color. We define two helper functions to convert from numeric to colors and vice-versa.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fTWftA_aWy",
        "outputId": "36ed6385-8e5a-462a-99da-4ed296ab934a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        }
      },
      "source": [
        "# ISPRS color palette\n",
        "# Let's define the standard ISPRS color palette\n",
        "palette = {0 : (255, 255, 255), # Impervious surfaces (white)\n",
        "           1 : (0, 0, 255),     # Buildings (blue)\n",
        "           2 : (0, 255, 255),   # Low vegetation (cyan)\n",
        "           3 : (0, 255, 0),     # Trees (green)\n",
        "           4 : (255, 255, 0),   # Cars (yellow)\n",
        "           5 : (255, 0, 0),     # Clutter (red)\n",
        "           6 : (0, 0, 0)}       # Undefined (black)\n",
        "\n",
        "invert_palette = {v: k for k, v in palette.items()}\n",
        "\n",
        "def convert_to_color(arr_2d, palette=palette):\n",
        "    \"\"\" Numeric labels to RGB-color encoding \"\"\"\n",
        "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
        "\n",
        "    for c, i in palette.items():\n",
        "        m = arr_2d == c\n",
        "        arr_3d[m] = i\n",
        "\n",
        "    return arr_3d\n",
        "\n",
        "def convert_from_color(arr_3d, palette=invert_palette):\n",
        "    \"\"\" RGB-color encoding to grayscale labels \"\"\"\n",
        "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    for c, i in palette.items():\n",
        "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
        "        arr_2d[m] = i\n",
        "\n",
        "    return arr_2d\n",
        "\n",
        "# We load one tile from the dataset and we display it\n",
        "print(MAIN_FOLDER+'top/top_mosaic_09cm_area1.tif')\n",
        "      \n",
        "img = io.imread(MAIN_FOLDER+'top/top_mosaic_09cm_area1.tif')\n",
        "fig = plt.figure()\n",
        "fig.add_subplot(121)\n",
        "\n",
        "print(img.shape)\n",
        "plt.imshow(img[:,:,2::1]);##img)\n",
        "\n",
        "# We load the ground truth\n",
        "gt = io.imread(MAIN_FOLDER+'gts_for_participants/top_mosaic_09cm_area1.tif')\n",
        "fig.add_subplot(122)\n",
        "plt.imshow(gt)\n",
        "plt.show()\n",
        "\n",
        "# We also check that we can convert the ground truth into an array format\n",
        "array_gt = convert_from_color(gt)\n",
        "print(\"Ground truth in numerical format has shape ({},{}) : \\n\".format(*array_gt.shape[:2]), array_gt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/JURSE/Vaihingen/top/top_mosaic_09cm_area1.tif\n",
            "(2569, 1919, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8306b406f1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;31m##img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# We load the ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2699\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2700\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    636\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    637\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAC7CAYAAADVEFpBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6xMCqbYANIb"
      },
      "source": [
        "We need to define a bunch of utils functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBs-fgNZAJc2"
      },
      "source": [
        "# Utils\n",
        "\n",
        "def get_random_pos(img, window_shape):\n",
        "    \"\"\" Extract of 2D random patch of shape window_shape in the image \"\"\"\n",
        "    w, h = window_shape\n",
        "    W, H = img.shape[-2:]\n",
        "    x1 = random.randint(0, W - w - 1)\n",
        "    x2 = x1 + w\n",
        "    y1 = random.randint(0, H - h - 1)\n",
        "    y2 = y1 + h\n",
        "    return x1, x2, y1, y2\n",
        "\n",
        "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
        "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
        "    dim = input.dim()\n",
        "    if dim == 2:\n",
        "        return F.cross_entropy(input, target, weight, size_average)\n",
        "##        return nn.CrossEntropyLoss(output, target,weight)\n",
        "    elif dim == 4:\n",
        "        output = input.view(input.size(0),input.size(1), -1)\n",
        "        output = torch.transpose(output,1,2).contiguous()\n",
        "        output = output.view(-1,output.size(2))\n",
        "        target = target.view(-1)\n",
        "        return F.cross_entropy(output, target,weight, size_average)\n",
        "##        return nn.CrossEntropyLoss(output, target,weight)\n",
        "    else:\n",
        "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
        "\n",
        "def accuracy(input, target):\n",
        "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
        "\n",
        "def sliding_window(top, step=10, window_size=(20,20)):\n",
        "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
        "    for x in range(0, top.shape[0], step):\n",
        "        if x + window_size[0] > top.shape[0]:\n",
        "            x = top.shape[0] - window_size[0]\n",
        "        for y in range(0, top.shape[1], step):\n",
        "            if y + window_size[1] > top.shape[1]:\n",
        "                y = top.shape[1] - window_size[1]\n",
        "            yield x, y, window_size[0], window_size[1]\n",
        "            \n",
        "def count_sliding_window(top, step=10, window_size=(20,20)):\n",
        "    \"\"\" Count the number of windows in an image \"\"\"\n",
        "    c = 0\n",
        "    for x in range(0, top.shape[0], step):\n",
        "        if x + window_size[0] > top.shape[0]:\n",
        "            x = top.shape[0] - window_size[0]\n",
        "        for y in range(0, top.shape[1], step):\n",
        "            if y + window_size[1] > top.shape[1]:\n",
        "                y = top.shape[1] - window_size[1]\n",
        "            c += 1\n",
        "    return c\n",
        "\n",
        "def grouper(n, iterable):\n",
        "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        chunk = tuple(itertools.islice(it, n))\n",
        "        if not chunk:\n",
        "            return\n",
        "        yield chunk\n",
        "\n",
        "def metrics(predictions, gts, label_values=LABELS):\n",
        "    cm = confusion_matrix(\n",
        "            gts,\n",
        "            predictions,\n",
        "            range(len(label_values)))\n",
        "    \n",
        "    print(\"Confusion matrix :\")\n",
        "    print(cm)\n",
        "    \n",
        "    print(\"---\")\n",
        "    \n",
        "    # Compute global accuracy\n",
        "    total = sum(sum(cm))\n",
        "    accuracy = sum([cm[x][x] for x in range(len(cm))])\n",
        "    accuracy *= 100 / float(total)\n",
        "    print(\"{} pixels processed\".format(total))\n",
        "    print(\"Total accuracy : {}%\".format(accuracy))\n",
        "    \n",
        "    print(\"---\")\n",
        "    \n",
        "    # Compute F1 score\n",
        "    F1Score = np.zeros(len(label_values))\n",
        "    for i in range(len(label_values)):\n",
        "        try:\n",
        "            F1Score[i] = 2. * cm[i,i] / (np.sum(cm[i,:]) + np.sum(cm[:,i]))\n",
        "        except:\n",
        "            # Ignore exception if there is no element in class i for test set\n",
        "            pass\n",
        "    print(\"F1Score :\")\n",
        "    for l_id, score in enumerate(F1Score):\n",
        "        print(\"{}: {}\".format(label_values[l_id], score))\n",
        "\n",
        "    print(\"---\")\n",
        "        \n",
        "    # Compute kappa coefficient\n",
        "    total = np.sum(cm)\n",
        "    pa = np.trace(cm) / float(total)\n",
        "    pe = np.sum(np.sum(cm, axis=0) * np.sum(cm, axis=1)) / float(total*total)\n",
        "    kappa = (pa - pe) / (1 - pe);\n",
        "    print(\"Kappa: \" + str(kappa))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v84piuYZAnRb"
      },
      "source": [
        "\n",
        "# Loading the dataset\n",
        "\n",
        "We define a PyTorch dataset (`torch.utils.data.Dataset)` that loads all the tiles in memory and performs random sampling. Tiles are stored in memory on the fly.\n",
        "\n",
        "The dataset also performs random data augmentation (horizontal and vertical flips) and normalizes the data in [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28W_ISbr_aaG"
      },
      "source": [
        "# Dataset class\n",
        "\n",
        "class ISPRS_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, data_files=DATA_FOLDER, label_files=LABEL_FOLDER,\n",
        "                            cache=False, augmentation=True):\n",
        "        super(ISPRS_dataset, self).__init__()\n",
        "        \n",
        "        self.augmentation = augmentation\n",
        "        self.cache = cache\n",
        "        \n",
        "        # List of files\n",
        "        self.data_files = [DATA_FOLDER.format(id) for id in ids]\n",
        "        self.label_files = [LABEL_FOLDER.format(id) for id in ids]\n",
        "\n",
        "        # Sanity check : raise an error if some files do not exist\n",
        "        for f in self.data_files + self.label_files:\n",
        "            if not os.path.isfile(f):\n",
        "                raise KeyError('{} is not a file !'.format(f))\n",
        "        \n",
        "        # Initialize cache dicts\n",
        "        self.data_cache_ = {}\n",
        "        self.label_cache_ = {}\n",
        "            \n",
        "    \n",
        "    def __len__(self):\n",
        "        # Default epoch size is 10 000 samples\n",
        "        return 10000\n",
        "    \n",
        "    @classmethod\n",
        "    def data_augmentation(cls, *arrays, flip=True, mirror=True):\n",
        "        will_flip, will_mirror = False, False\n",
        "        if flip and random.random() < 0.5:\n",
        "            will_flip = True\n",
        "        if mirror and random.random() < 0.5:\n",
        "            will_mirror = True\n",
        "        \n",
        "        results = []\n",
        "        for array in arrays:\n",
        "            if will_flip:\n",
        "                if len(array.shape) == 2:\n",
        "                    array = array[::-1, :]\n",
        "                else:\n",
        "                    array = array[:, ::-1, :]\n",
        "            if will_mirror:\n",
        "                if len(array.shape) == 2:\n",
        "                    array = array[:, ::-1]\n",
        "                else:\n",
        "                    array = array[:, :, ::-1]\n",
        "            results.append(np.copy(array))\n",
        "            \n",
        "        return tuple(results)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        # Pick a random image\n",
        "        random_idx = random.randint(0, len(self.data_files) - 1)\n",
        "        \n",
        "        # If the tile hasn't been loaded yet, put in cache\n",
        "        if random_idx in self.data_cache_.keys():\n",
        "            data = self.data_cache_[random_idx]\n",
        "        else:\n",
        "            # Data is normalized in [0, 1]\n",
        "            data = 1/255 * np.asarray(io.imread(self.data_files[random_idx]).transpose((2,0,1)), dtype='float32')\n",
        "            if self.cache:\n",
        "                self.data_cache_[random_idx] = data\n",
        "            \n",
        "        if random_idx in self.label_cache_.keys():\n",
        "            label = self.label_cache_[random_idx]\n",
        "        else: \n",
        "            # Labels are converted from RGB to their numeric values\n",
        "            label = np.asarray(convert_from_color(io.imread(self.label_files[random_idx])), dtype='int64')\n",
        "            if self.cache:\n",
        "                self.label_cache_[random_idx] = label\n",
        "\n",
        "        # Get a random patch\n",
        "        x1, x2, y1, y2 = get_random_pos(data, WINDOW_SIZE)\n",
        "        data_p = data[:, x1:x2,y1:y2]\n",
        "        label_p = label[x1:x2,y1:y2]\n",
        "        \n",
        "        # Data augmentation\n",
        "        data_p, label_p = self.data_augmentation(data_p, label_p)\n",
        "\n",
        "        # Return the torch.Tensor values\n",
        "        return (torch.from_numpy(data_p),\n",
        "                torch.from_numpy(label_p))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2KmX78VA8yR"
      },
      "source": [
        "# Network definition \n",
        "\n",
        "We can now define the Fully Convolutional network based on the SegNet architecture. We could use any other network as drop-in replacement, provided that the output has dimensions (`N_CLASSES, W, H`) where `W` and `H` are the sliding window dimensions (i.e. the network should preserve the spatial dimensions).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/1e568c27d091bdf210e4c71da77289dde321b47a/68747470733a2f2f6672616d617069632e6f72672f51634b4d4b4e6f53334e4e782f4861563065736c6669783650\" alt=\"SegNet arrchitecture\"  width=600/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf31vjH7BLbD"
      },
      "source": [
        "\n",
        "class SegNet(nn.Module):\n",
        "    # SegNet network\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.kaiming_normal(m.weight.data)\n",
        "    \n",
        "    def __init__(self, in_channels=IN_CHANNELS, out_channels=N_CLASSES):\n",
        "        super(SegNet, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
        "        self.unpool = nn.MaxUnpool2d(2)\n",
        "        \n",
        "        self.conv1_1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.conv1_1_bn = nn.BatchNorm2d(64)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv1_2_bn = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2_1_bn = nn.BatchNorm2d(128)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv2_2_bn = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_1_bn = nn.BatchNorm2d(256)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv3_2_bn = nn.BatchNorm2d(256)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv3_3_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv4_1_bn = nn.BatchNorm2d(512)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv4_2_bn = nn.BatchNorm2d(512)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv4_3_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_1_bn = nn.BatchNorm2d(512)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_2_bn = nn.BatchNorm2d(512)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_3_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv5_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_3_D_bn = nn.BatchNorm2d(512)\n",
        "        self.conv5_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_2_D_bn = nn.BatchNorm2d(512)\n",
        "        self.conv5_1_D = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv5_1_D_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv4_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv4_3_D_bn = nn.BatchNorm2d(512)\n",
        "        self.conv4_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv4_2_D_bn = nn.BatchNorm2d(512)\n",
        "        self.conv4_1_D = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.conv4_1_D_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv3_3_D = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv3_3_D_bn = nn.BatchNorm2d(256)\n",
        "        self.conv3_2_D = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv3_2_D_bn = nn.BatchNorm2d(256)\n",
        "        self.conv3_1_D = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.conv3_1_D_bn = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv2_2_D = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv2_2_D_bn = nn.BatchNorm2d(128)\n",
        "        self.conv2_1_D = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.conv2_1_D_bn = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv1_2_D = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv1_2_D_bn = nn.BatchNorm2d(64)\n",
        "        self.conv1_1_D = nn.Conv2d(64, out_channels, 3, padding=1)\n",
        "        \n",
        "        self.apply(self.weight_init)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder block 1\n",
        "        x = self.conv1_1_bn(F.relu(self.conv1_1(x)))\n",
        "        x = self.conv1_2_bn(F.relu(self.conv1_2(x)))\n",
        "        x, mask1 = self.pool(x)\n",
        "        \n",
        "        # Encoder block 2\n",
        "        x = self.conv2_1_bn(F.relu(self.conv2_1(x)))\n",
        "        x = self.conv2_2_bn(F.relu(self.conv2_2(x)))\n",
        "        x, mask2 = self.pool(x)\n",
        "        \n",
        "        # Encoder block 3\n",
        "        x = self.conv3_1_bn(F.relu(self.conv3_1(x)))\n",
        "        x = self.conv3_2_bn(F.relu(self.conv3_2(x)))\n",
        "        x = self.conv3_3_bn(F.relu(self.conv3_3(x)))\n",
        "        x, mask3 = self.pool(x)\n",
        "        \n",
        "        # Encoder block 4\n",
        "        x = self.conv4_1_bn(F.relu(self.conv4_1(x)))\n",
        "        x = self.conv4_2_bn(F.relu(self.conv4_2(x)))\n",
        "        x = self.conv4_3_bn(F.relu(self.conv4_3(x)))\n",
        "        x, mask4 = self.pool(x)\n",
        "        \n",
        "        # Encoder block 5\n",
        "        x = self.conv5_1_bn(F.relu(self.conv5_1(x)))\n",
        "        x = self.conv5_2_bn(F.relu(self.conv5_2(x)))\n",
        "        x = self.conv5_3_bn(F.relu(self.conv5_3(x)))\n",
        "        x, mask5 = self.pool(x)\n",
        "        \n",
        "        # Decoder block 5\n",
        "        x = self.unpool(x, mask5)\n",
        "        x = self.conv5_3_D_bn(F.relu(self.conv5_3_D(x)))\n",
        "        x = self.conv5_2_D_bn(F.relu(self.conv5_2_D(x)))\n",
        "        x = self.conv5_1_D_bn(F.relu(self.conv5_1_D(x)))\n",
        "        \n",
        "        # Decoder block 4\n",
        "        x = self.unpool(x, mask4)\n",
        "        x = self.conv4_3_D_bn(F.relu(self.conv4_3_D(x)))\n",
        "        x = self.conv4_2_D_bn(F.relu(self.conv4_2_D(x)))\n",
        "        x = self.conv4_1_D_bn(F.relu(self.conv4_1_D(x)))\n",
        "        \n",
        "        # Decoder block 3\n",
        "        x = self.unpool(x, mask3)\n",
        "        x = self.conv3_3_D_bn(F.relu(self.conv3_3_D(x)))\n",
        "        x = self.conv3_2_D_bn(F.relu(self.conv3_2_D(x)))\n",
        "        x = self.conv3_1_D_bn(F.relu(self.conv3_1_D(x)))\n",
        "        \n",
        "        # Decoder block 2\n",
        "        x = self.unpool(x, mask2)\n",
        "        x = self.conv2_2_D_bn(F.relu(self.conv2_2_D(x)))\n",
        "        x = self.conv2_1_D_bn(F.relu(self.conv2_1_D(x)))\n",
        "        \n",
        "        # Decoder block 1\n",
        "        x = self.unpool(x, mask1)\n",
        "        x = self.conv1_2_D_bn(F.relu(self.conv1_2_D(x)))\n",
        "        x = F.log_softmax(self.conv1_1_D(x))\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLgB8FGQC3XW"
      },
      "source": [
        "We can now instantiate the network using the specified parameters. By default, the weights will be initialized using the [He policy](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) (i.e. zero-mean Gaussian distribution whose standard deviation is `√(2/n) `for a layer with `n` response connections and `b=0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nANwqZMbBLky"
      },
      "source": [
        "# instantiate the network\n",
        "net = SegNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RyhKsTuDufZ"
      },
      "source": [
        "We download and load the pre-trained weights from VGG-16 on ImageNet. This step is optional but it makes the network converge faster. We skip the weights from VGG-16 that have no counterpart in SegNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRFasMmODxDA",
        "outputId": "455def9f-7cc9-412d-c7e7-b457afd5b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1416
        }
      },
      "source": [
        "import os\n",
        "try:\n",
        "    from urllib.request import URLopener\n",
        "except ImportError:\n",
        "    from urllib import URLopener\n",
        "\n",
        "# Download VGG-16 weights from PyTorch\n",
        "vgg_url = 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth'\n",
        "if not os.path.isfile('./vgg16_bn-6c64b313.pth'):\n",
        "    weights = URLopener().retrieve(vgg_url, './vgg16_bn-6c64b313.pth')\n",
        "\n",
        "vgg16_weights = torch.load('./vgg16_bn-6c64b313.pth')\n",
        "mapped_weights = {}\n",
        "for k_vgg, k_segnet in zip(vgg16_weights.keys(), net.state_dict().keys()):\n",
        "    if \"features\" in k_vgg:\n",
        "        mapped_weights[k_segnet] = vgg16_weights[k_vgg]\n",
        "        print(\"Mapping {} to {}\".format(k_vgg, k_segnet))\n",
        "        \n",
        "try:\n",
        "    net.load_state_dict(mapped_weights)\n",
        "    print(\"Loaded VGG-16 weights in SegNet !\")\n",
        "except:\n",
        "    # Ignore missing keys\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mapping features.0.weight to conv1_1.weight\n",
            "Mapping features.0.bias to conv1_1.bias\n",
            "Mapping features.1.weight to conv1_1_bn.weight\n",
            "Mapping features.1.bias to conv1_1_bn.bias\n",
            "Mapping features.1.running_mean to conv1_1_bn.running_mean\n",
            "Mapping features.1.running_var to conv1_1_bn.running_var\n",
            "Mapping features.3.weight to conv1_1_bn.num_batches_tracked\n",
            "Mapping features.3.bias to conv1_2.weight\n",
            "Mapping features.4.weight to conv1_2.bias\n",
            "Mapping features.4.bias to conv1_2_bn.weight\n",
            "Mapping features.4.running_mean to conv1_2_bn.bias\n",
            "Mapping features.4.running_var to conv1_2_bn.running_mean\n",
            "Mapping features.7.weight to conv1_2_bn.running_var\n",
            "Mapping features.7.bias to conv1_2_bn.num_batches_tracked\n",
            "Mapping features.8.weight to conv2_1.weight\n",
            "Mapping features.8.bias to conv2_1.bias\n",
            "Mapping features.8.running_mean to conv2_1_bn.weight\n",
            "Mapping features.8.running_var to conv2_1_bn.bias\n",
            "Mapping features.10.weight to conv2_1_bn.running_mean\n",
            "Mapping features.10.bias to conv2_1_bn.running_var\n",
            "Mapping features.11.weight to conv2_1_bn.num_batches_tracked\n",
            "Mapping features.11.bias to conv2_2.weight\n",
            "Mapping features.11.running_mean to conv2_2.bias\n",
            "Mapping features.11.running_var to conv2_2_bn.weight\n",
            "Mapping features.14.weight to conv2_2_bn.bias\n",
            "Mapping features.14.bias to conv2_2_bn.running_mean\n",
            "Mapping features.15.weight to conv2_2_bn.running_var\n",
            "Mapping features.15.bias to conv2_2_bn.num_batches_tracked\n",
            "Mapping features.15.running_mean to conv3_1.weight\n",
            "Mapping features.15.running_var to conv3_1.bias\n",
            "Mapping features.17.weight to conv3_1_bn.weight\n",
            "Mapping features.17.bias to conv3_1_bn.bias\n",
            "Mapping features.18.weight to conv3_1_bn.running_mean\n",
            "Mapping features.18.bias to conv3_1_bn.running_var\n",
            "Mapping features.18.running_mean to conv3_1_bn.num_batches_tracked\n",
            "Mapping features.18.running_var to conv3_2.weight\n",
            "Mapping features.20.weight to conv3_2.bias\n",
            "Mapping features.20.bias to conv3_2_bn.weight\n",
            "Mapping features.21.weight to conv3_2_bn.bias\n",
            "Mapping features.21.bias to conv3_2_bn.running_mean\n",
            "Mapping features.21.running_mean to conv3_2_bn.running_var\n",
            "Mapping features.21.running_var to conv3_2_bn.num_batches_tracked\n",
            "Mapping features.24.weight to conv3_3.weight\n",
            "Mapping features.24.bias to conv3_3.bias\n",
            "Mapping features.25.weight to conv3_3_bn.weight\n",
            "Mapping features.25.bias to conv3_3_bn.bias\n",
            "Mapping features.25.running_mean to conv3_3_bn.running_mean\n",
            "Mapping features.25.running_var to conv3_3_bn.running_var\n",
            "Mapping features.27.weight to conv3_3_bn.num_batches_tracked\n",
            "Mapping features.27.bias to conv4_1.weight\n",
            "Mapping features.28.weight to conv4_1.bias\n",
            "Mapping features.28.bias to conv4_1_bn.weight\n",
            "Mapping features.28.running_mean to conv4_1_bn.bias\n",
            "Mapping features.28.running_var to conv4_1_bn.running_mean\n",
            "Mapping features.30.weight to conv4_1_bn.running_var\n",
            "Mapping features.30.bias to conv4_1_bn.num_batches_tracked\n",
            "Mapping features.31.weight to conv4_2.weight\n",
            "Mapping features.31.bias to conv4_2.bias\n",
            "Mapping features.31.running_mean to conv4_2_bn.weight\n",
            "Mapping features.31.running_var to conv4_2_bn.bias\n",
            "Mapping features.34.weight to conv4_2_bn.running_mean\n",
            "Mapping features.34.bias to conv4_2_bn.running_var\n",
            "Mapping features.35.weight to conv4_2_bn.num_batches_tracked\n",
            "Mapping features.35.bias to conv4_3.weight\n",
            "Mapping features.35.running_mean to conv4_3.bias\n",
            "Mapping features.35.running_var to conv4_3_bn.weight\n",
            "Mapping features.37.weight to conv4_3_bn.bias\n",
            "Mapping features.37.bias to conv4_3_bn.running_mean\n",
            "Mapping features.38.weight to conv4_3_bn.running_var\n",
            "Mapping features.38.bias to conv4_3_bn.num_batches_tracked\n",
            "Mapping features.38.running_mean to conv5_1.weight\n",
            "Mapping features.38.running_var to conv5_1.bias\n",
            "Mapping features.40.weight to conv5_1_bn.weight\n",
            "Mapping features.40.bias to conv5_1_bn.bias\n",
            "Mapping features.41.weight to conv5_1_bn.running_mean\n",
            "Mapping features.41.bias to conv5_1_bn.running_var\n",
            "Mapping features.41.running_mean to conv5_1_bn.num_batches_tracked\n",
            "Mapping features.41.running_var to conv5_2.weight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQptXR6kH5Yx"
      },
      "source": [
        "\n",
        "Then, we load the network on GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnQ-_dF-ILje",
        "outputId": "cbe35adb-cdc5-4fac-f140-fdbad490261d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "source": [
        "net.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SegNet(\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_1_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_1_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_3_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_3_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_2_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_2_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5_1_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_1_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_3_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_3_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_2_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_2_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4_1_D): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_1_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_3_D): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_3_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_2_D): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_2_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_1_D): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_1_D_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_2_D): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_2_D_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_1_D): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_1_D_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv1_2_D): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_2_D_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv1_1_D): Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhuWCc2cI43p"
      },
      "source": [
        "\n",
        "# Loading the data\n",
        "\n",
        "We now create a train/test split. If you want to use another dataset, you have to adjust the method to collect all filenames. In our case, we specify a fixed train/test split for the demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAKhuI8t9MWC",
        "outputId": "fa43a5c9-7136-4fa5-e488-8d667edc853c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Load the datasets\n",
        "if DATASET == 'Potsdam':\n",
        "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
        "    all_ids = [\"\".join(f.split('')[5:7]) for f in all_files]\n",
        "elif DATASET == 'Vaihingen':\n",
        "    #all_ids = \n",
        "    all_files = sorted(glob(LABEL_FOLDER.replace('{}', '*')))\n",
        "    all_ids = [f.split('area')[-1].split('.')[0] for f in all_files]\n",
        "# Random tile numbers for train/test split\n",
        "train_ids = random.sample(all_ids, 2 * len(all_ids) // 3 + 1)\n",
        "test_ids = list(set(all_ids) - set(train_ids))\n",
        "\n",
        "# Exemple of a train/val split on Vaihingen :\n",
        "##train_ids = ['1', '3', '23', '26', '7', '11', '13', '28', '17', '32', '34', '37']\n",
        "##test_ids = ['5', '21', '15', '30'] \n",
        "train_ids = ['1', '23']\n",
        "test_ids = ['5'] \n",
        "\n",
        "\n",
        "\n",
        "print(\"Tiles for training : \", train_ids)\n",
        "print(\"Tiles for testing : \", test_ids)\n",
        "\n",
        "train_set = ISPRS_dataset(train_ids, cache=CACHE)\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b449ad4cf4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mDATASET\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Potsdam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_FOLDER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yODkcni5JC1o"
      },
      "source": [
        "\n",
        "# Designing the optimizer\n",
        "\n",
        "We use the standard Stochastic Gradient Descent algorithm to optimize the network's weights.\n",
        "\n",
        "The encoder is trained at half the learning rate of the decoder, as we rely on the pre-trained VGG-16 weights. We use the `torch.optim.lr_scheduler` to reduce the learning rate by 10 after 25, 35 and 45 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvEqRB6wI6pX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "27928228-8683-481f-d417-6cf5ab1cec16"
      },
      "source": [
        "base_lr = 0.01\n",
        "params_dict = dict(net.named_parameters())\n",
        "params = []\n",
        "for key, value in params_dict.items():\n",
        "    if '_D' in key:\n",
        "        # Decoder weights are trained at the nominal learning rate\n",
        "        params += [{'params':[value],'lr': base_lr}]\n",
        "    else:\n",
        "        # Encoder weights are trained at lr / 2 (we have VGG-16 weights as initialization)\n",
        "        params += [{'params':[value],'lr': base_lr / 2}]\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0005)\n",
        "# We define the scheduler\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [25, 35, 45], gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-97a77078ecf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'_D'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcxL0DQIJVtg"
      },
      "source": [
        "def test(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
        "    # Use the network on the test set\n",
        "    test_images = (1 / 255 * np.asarray(io.imread(DATA_FOLDER.format(id)), dtype='float32') for id in test_ids)\n",
        "    test_labels = (np.asarray(io.imread(LABEL_FOLDER.format(id)), dtype='uint8') for id in test_ids)\n",
        "    eroded_labels = (convert_from_color(io.imread(ERODED_FOLDER.format(id))) for id in test_ids)\n",
        "    all_preds = []\n",
        "    all_gts = []\n",
        "    \n",
        "    # Switch the network to inference mode\n",
        "    net.eval()\n",
        "\n",
        "    for img, gt, gt_e in tqdm(zip(test_images, test_labels, eroded_labels), total=len(test_ids), leave=False):\n",
        "        pred = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
        "\n",
        "        total = count_sliding_window(img, step=stride, window_size=window_size) // batch_size\n",
        "        for i, coords in enumerate(tqdm(grouper(batch_size, sliding_window(img, step=stride, window_size=window_size)), total=total, leave=False)):\n",
        "            # Display in progress results\n",
        "            if i > 0 and total > 10 and i % int(10 * total / 100) == 0:\n",
        "                    _pred = np.argmax(pred, axis=-1)\n",
        "                    fig = plt.figure()\n",
        "                    fig.add_subplot(1,3,1)\n",
        "                    plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
        "                    fig.add_subplot(1,3,2)\n",
        "                    plt.imshow(convert_to_color(_pred))\n",
        "                    fig.add_subplot(1,3,3)\n",
        "                    plt.imshow(gt)\n",
        "                    clear_output()\n",
        "                    plt.show()\n",
        "                    \n",
        "            # Build the tensor\n",
        "            image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n",
        "            image_patches = np.asarray(image_patches)\n",
        "            image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
        "            \n",
        "            # Do the inference\n",
        "            outs = net(image_patches)\n",
        "            outs = outs.data.cpu().numpy()\n",
        "            \n",
        "            # Fill in the results array\n",
        "            for out, (x, y, w, h) in zip(outs, coords):\n",
        "                out = out.transpose((1,2,0))\n",
        "                pred[x:x+w, y:y+h] += out\n",
        "            del(outs)\n",
        "\n",
        "        pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "        # Display the result\n",
        "        clear_output()\n",
        "        fig = plt.figure()\n",
        "        fig.add_subplot(1,3,1)\n",
        "        plt.imshow(np.asarray(255 * img, dtype='uint8'))\n",
        "        fig.add_subplot(1,3,2)\n",
        "        plt.imshow(convert_to_color(pred))\n",
        "        fig.add_subplot(1,3,3)\n",
        "        plt.imshow(gt)\n",
        "        plt.show()\n",
        "\n",
        "        all_preds.append(pred)\n",
        "        all_gts.append(gt_e)\n",
        "\n",
        "        clear_output()\n",
        "        # Compute some metrics\n",
        "        metrics(pred.ravel(), gt_e.ravel())\n",
        "        accuracy = metrics(np.concatenate([p.ravel() for p in all_preds]), np.concatenate([p.ravel() for p in all_gts]).ravel())\n",
        "    if all:\n",
        "        return accuracy, all_preds, all_gts\n",
        "    else:\n",
        "        return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhILeXKGJV4s"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train(net, optimizer, epochs, scheduler=None, weights=WEIGHTS, save_epoch = 5):\n",
        "    losses = np.zeros(1000000)\n",
        "    mean_losses = np.zeros(100000000)\n",
        "    weights = weights.cuda()\n",
        "\n",
        "    criterion = nn.NLLLoss2d(weight=weights)\n",
        "    iter_ = 0\n",
        "    \n",
        "    for e in range(1, epochs + 1):\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        net.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "            optimizer.zero_grad()\n",
        "            output = net(data)\n",
        "            loss = CrossEntropy2d(output, target, weight=weights)\n",
        "##            loss = F.cross_entropy(output, target, weight=weights)            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            \n",
        "            losses[iter_] = loss.item() ##loss.data[0]\n",
        "            mean_losses[iter_] = np.mean(losses[max(0,iter_-100):iter_])\n",
        "            \n",
        "            if iter_ % 100 == 0:\n",
        "                clear_output()\n",
        "                rgb = np.asarray(255 * np.transpose(data.data.cpu().numpy()[0],(1,2,0)), dtype='uint8')\n",
        "                pred = np.argmax(output.data.cpu().numpy()[0], axis=0)\n",
        "                gt = target.data.cpu().numpy()[0]\n",
        "                print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}'.format(\n",
        "                    e, epochs, batch_idx, len(train_loader),\n",
        "                    100. * batch_idx / len(train_loader), loss.item(), accuracy(pred, gt))) ##loss.data[0]\n",
        "                plt.plot(mean_losses[:iter_]) and plt.show()\n",
        "                fig = plt.figure()\n",
        "                fig.add_subplot(131)\n",
        "                plt.imshow(rgb)\n",
        "                plt.title('RGB')\n",
        "                fig.add_subplot(132)\n",
        "                plt.imshow(convert_to_color(gt))\n",
        "                plt.title('Ground truth')\n",
        "                fig.add_subplot(133)\n",
        "                plt.title('Prediction')\n",
        "                plt.imshow(convert_to_color(pred))\n",
        "                plt.show()\n",
        "            iter_ += 1\n",
        "            \n",
        "            del(data, target, loss)\n",
        "            \n",
        "        #if e % save_epoch == 0:\n",
        "            # We validate with the largest possible stride for faster computing\n",
        "        acc = test(net, test_ids, all=False, stride=min(WINDOW_SIZE))\n",
        "            #torch.save(net.state_dict(), './segnet256_epoch{}_{}'.format(e, acc))\n",
        "    torch.save(net.state_dict(), './segnet_final')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_9f-KgJPFk"
      },
      "source": [
        "# Training the network\n",
        "\n",
        "Let's train the network for 1 epoch to see how it works (back at work, better train for 50 epochs). The `matplotlib` graph is periodically udpated with the loss plot and a sample inference. It might takes a few minutes on GPUs in the cloud.\n",
        "\n",
        "If using the notebook on your own machine with the full 50 epochs, depending on your GPU, this might take from a few hours (Titan Pascal) to a full day (old K20).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNblwA_JV-B",
        "outputId": "942ee351-36c9-4354-cb75-8e83f17c006c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        }
      },
      "source": [
        "train(net, optimizer, 1, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix :\n",
            "[[1362848  132381   17236    1219    2113       0]\n",
            " [ 370876 1881133   47215    1082    2244       0]\n",
            " [  80279    5061   86795   91560     114       0]\n",
            " [  11946     195   19846  260649       0       0]\n",
            " [  38501    4426     541      95    1383       0]\n",
            " [      0       0       0       0       0       0]]\n",
            "---\n",
            "4419738 pixels processed\n",
            "Total accuracy : 81.290067420286%\n",
            "---\n",
            "F1Score :\n",
            "roads: 0.8063600086029217\n",
            "buildings: 0.8697380752360402\n",
            "low veg.: 0.3986524037644508\n",
            "trees: 0.8054156025344501\n",
            "cars: 0.054448818897637793\n",
            "clutter: nan\n",
            "---\n",
            "Kappa: 0.6928762539481693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix :\n",
            "[[1362848  132381   17236    1219    2113       0]\n",
            " [ 370876 1881133   47215    1082    2244       0]\n",
            " [  80279    5061   86795   91560     114       0]\n",
            " [  11946     195   19846  260649       0       0]\n",
            " [  38501    4426     541      95    1383       0]\n",
            " [      0       0       0       0       0       0]]\n",
            "---\n",
            "4419738 pixels processed\n",
            "Total accuracy : 81.290067420286%\n",
            "---\n",
            "F1Score :\n",
            "roads: 0.8063600086029217\n",
            "buildings: 0.8697380752360402\n",
            "low veg.: 0.3986524037644508\n",
            "trees: 0.8054156025344501\n",
            "cars: 0.054448818897637793\n",
            "clutter: nan\n",
            "---\n",
            "Kappa: 0.6928762539481693\n",
            "\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxf4xSjAePhn"
      },
      "source": [
        "\n",
        "# Testing the network\n",
        "\n",
        "Now that the training has ended, we can load the final weights and test the network using a reasonable stride, e.g. half or a quarter of the window size. Inference time depends on the chosen stride, e.g. a step size of 32 (75% overlap) will take 10 secondes / image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2C345YCQ8U7",
        "outputId": "e2587156-c782-4f99-8cfa-cc3bc26828f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "net.load_state_dict(torch.load('./segnet_final'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UqKC5TJXnu2",
        "outputId": "64343f68-1172-4438-f65d-89c5c5759c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        }
      },
      "source": [
        "_, all_preds, all_gts = test(net, test_ids, all=True, stride=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix :\n",
            "[[1366256  134158   12523    1376    1484       0]\n",
            " [ 360561 1901021   38970     844    1154       0]\n",
            " [  78275    5873   87331   92302      28       0]\n",
            " [  11390     181   18349  262716       0       0]\n",
            " [  39269    4275     394      45     963       0]\n",
            " [      0       0       0       0       0       0]]\n",
            "---\n",
            "4419738 pixels processed\n",
            "Total accuracy : 81.86654955565238%\n",
            "---\n",
            "F1Score :\n",
            "roads: 0.8104621378666417\n",
            "buildings: 0.8744230182762052\n",
            "low veg.: 0.4145039109963548\n",
            "trees: 0.8084576693403331\n",
            "cars: 0.039650025733401956\n",
            "clutter: nan\n",
            "---\n",
            "Kappa: 0.7014571050771584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix :\n",
            "[[1366256  134158   12523    1376    1484       0]\n",
            " [ 360561 1901021   38970     844    1154       0]\n",
            " [  78275    5873   87331   92302      28       0]\n",
            " [  11390     181   18349  262716       0       0]\n",
            " [  39269    4275     394      45     963       0]\n",
            " [      0       0       0       0       0       0]]\n",
            "---\n",
            "4419738 pixels processed\n",
            "Total accuracy : 81.86654955565238%\n",
            "---\n",
            "F1Score :\n",
            "roads: 0.8104621378666417\n",
            "buildings: 0.8744230182762052\n",
            "low veg.: 0.4145039109963548\n",
            "trees: 0.8084576693403331\n",
            "cars: 0.039650025733401956\n",
            "clutter: nan\n",
            "---\n",
            "Kappa: 0.7014571050771584\n",
            "\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R47BdtUjeKpF"
      },
      "source": [
        "\n",
        "# Saving the results\n",
        "\n",
        "We can visualize and save the resulting tiles for qualitative assessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ICWhJOXwiu",
        "outputId": "4cefd2c4-3767-43c2-a84f-9f63e8662e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "for p, id_ in zip(all_preds, test_ids):\n",
        "    img = convert_to_color(p)\n",
        "    plt.imshow(img) and plt.show()\n",
        "    io.imsave('./inference_tile_{}.png'.format(id_), img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAD8CAYAAAACP/oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}